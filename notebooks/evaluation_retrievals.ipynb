{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ffd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- your bedrock model (as you specified) ---\n",
    "judge_llm = init_chat_model(\n",
    "    # \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    \"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "        # \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    "    # top_p=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load ---\n",
    "df  = pd.read_csv(\"exp1_300m_rerank600m_rev_eval.csv\")[[\"prompt\", \"recommendations\"]]\n",
    "exp = pd.read_csv(\"final_benchmark.csv\")\n",
    "\n",
    "# --- Ensure we have Clarification Ground Truth; create blank if missing ---\n",
    "clar_col = None\n",
    "for c in exp.columns:\n",
    "    if c.strip().lower() == \"clarification ground truth\":\n",
    "        clar_col = c\n",
    "        break\n",
    "if clar_col is None:\n",
    "    clar_col = \"Clarification Ground Truth\"\n",
    "    exp[clar_col] = \"\"\n",
    "\n",
    "# Keep only needed cols from exp\n",
    "exp = exp[[\"query\", clar_col]]\n",
    "\n",
    "# --- Build canonical keys for robust matching (strip, collapse spaces, lowercase) ---\n",
    "def canon(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "         .str.lower()\n",
    "    )\n",
    "\n",
    "exp[\"_key\"] = canon(exp[\"query\"])\n",
    "df[\"_key\"]  = canon(df[\"prompt\"])\n",
    "\n",
    "# --- Collapse duplicates on the right (df) so the merge is many-to-one ---\n",
    "# Keep first occurrence; rename recommendations -> answer\n",
    "df_uni = (\n",
    "    df.sort_index()\n",
    "      .drop_duplicates(subset=[\"_key\"], keep=\"first\")\n",
    "      .rename(columns={\"recommendations\": \"answer\"})\n",
    ")\n",
    "\n",
    "# --- Left-merge from exp -> df_uni; preserves exactly len(exp) rows (214) ---\n",
    "merged = exp.merge(\n",
    "    df_uni[[\"_key\", \"prompt\", \"answer\"]],\n",
    "    on=\"_key\",\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"\n",
    ").drop(columns=[\"_key\"])\n",
    "\n",
    "# --- Final columns & optional fill for missing matches ---\n",
    "final = merged[[\"query\", \"prompt\", \"answer\", clar_col]].fillna({\"prompt\": \"\", \"answer\": \"\"})\n",
    "\n",
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Mapping, Optional\n",
    "\n",
    "# EXACT header copied from your function (unchanged)\n",
    "RELEVANCE_HEADER = (\n",
    "    \"You are a Relevance Judge for HOTEL RECOMMENDATIONS.\\n\"\n",
    "    \"Evaluate ONLY using the provided hotel descriptions and reviews (ignore any outside knowledge).\\n\\n\"\n",
    "    \"Task: Rate how well the ANSWER addresses the USER QUERY on a 1–5 scale:\\n\"\n",
    "    \"1 = Not relevant at all — completely misses the user's needs.\\n\"\n",
    "    \"2 = Slightly relevant — touches minor aspects but not the core requirements.\\n\"\n",
    "    \"3 = Moderately relevant — addresses some key points but misses important requirements.\\n\"\n",
    "    \"4 = Very relevant — covers most requirements well, with minor omissions.\\n\"\n",
    "    \"5 = Perfectly relevant — comprehensively addresses all requirements with appropriate detail.\\n\\n\"\n",
    "    \"When evaluating, consider:\\n\"\n",
    "    \"• The answer represents the top 3 retrieved documents corresponding to the top 3 hotel candidates — these documents are raw text.\\n\"\n",
    "    \"• Focus on evaluating the knowledge contained in these documents and how well it satisfies the user's stated requirements, rather than the presentation quality or completeness of the text.\\n\\n\"\n",
    "        \n",
    "    \"Output format: Return ONLY a valid JSON object with two fields:\"\n",
    "    \"- score: an integer from 1 to 5\"\n",
    "    \"- explanation: a brief explanation for the chosen score\"\n",
    "    \"Example:\\n\"\n",
    "    '{\\n  \"score\": 4,\\n  \"explanation\": \"The answer is mostly correct and relevant but misses a minor detail.\"\\n}'\n",
    "    \"Do not include any text outside the JSON object.\"\n",
    ")\n",
    "\n",
    "def _nz(x: Optional[str]) -> str:\n",
    "    return (\"\" if x is None else str(x)).strip()\n",
    "\n",
    "def build_prompt_exact(\n",
    "    row: Mapping,\n",
    "    query_col: str = \"query\",\n",
    "    answer_col: str = \"answer\",\n",
    "    clar_col: str = \"Clarification Ground Truth\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the prompt using your EXACT header, plus:\n",
    "      - User's query\n",
    "      - Answer\n",
    "      - Clarification of the user\n",
    "    \"\"\"\n",
    "    query = _nz(row.get(query_col))\n",
    "    answer = _nz(row.get(answer_col))\n",
    "    clar = _nz(row.get(clar_col)) or \"(no implicit explanation provided)\"\n",
    "\n",
    "    final_prompt = (\n",
    "        f\"{RELEVANCE_HEADER}\\n\\n\"\n",
    "        \"**********\\n\"\n",
    "        \"User's query:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"**********\\n\"\n",
    "        \"Answer:\\n\"\n",
    "        f\"{answer}\\n\"\n",
    "        \"**********\\n\"\n",
    "        \"Clarification of the user:\\n\"\n",
    "        f\"{clar}\\n\"\n",
    "    )\n",
    "    return final_prompt\n",
    "\n",
    "def add_prompt_column_exact(\n",
    "    df: pd.DataFrame,\n",
    "    query_col: str = \"query\",\n",
    "    answer_col: str = \"answer\",\n",
    "    clar_col: str = \"Clarification Ground Truth\",\n",
    "    new_col: str = \"prompt\"\n",
    ") -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[new_col] = out.apply(\n",
    "        lambda r: build_prompt_exact(r, query_col, answer_col, clar_col),\n",
    "        axis=1\n",
    "    )\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c812042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_json_maybe(s: str):\n",
    "    # strip optional ```json fences\n",
    "    clean = s.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "    try:\n",
    "        return json.loads(clean)\n",
    "    except Exception:\n",
    "        return clean  # fall back to raw text\n",
    "\n",
    "# If judge_llm isn't thread-safe, uncomment the lock below.\n",
    "# from threading import Lock\n",
    "# _invoke_lock = Lock()\n",
    "\n",
    "def call_judge(i, prompt):\n",
    "    try:\n",
    "        # If not thread-safe, wrap the next line with the lock:\n",
    "        # with _invoke_lock:\n",
    "        answer = judge_llm.invoke(prompt)\n",
    "        text = getattr(answer, \"content\", str(answer))\n",
    "        return i, parse_json_maybe(text)\n",
    "    except Exception as e:\n",
    "        # return the error payload so you can inspect failures later\n",
    "        return i, {\"error\": str(e)}\n",
    "\n",
    "evaluations = []\n",
    "final_with_prompts = add_prompt_column_exact(final)\n",
    "prompts = final_with_prompts[\"prompt\"].tolist()\n",
    "\n",
    "evaluations = [None] * len(prompts)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as ex:\n",
    "    futures = [ex.submit(call_judge, i, p) for i, p in enumerate(prompts)]\n",
    "    for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Judging\"):\n",
    "        i, result = fut.result()\n",
    "        evaluations[i] = result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55127da",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17df3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_corpus = pd.read_csv(\"hotels.csv\", encoding=\"latin1\")\n",
    "df_corpus.columns = df_corpus.columns.str.strip()\n",
    "\n",
    "df_corpus[\"combined\"] = (\n",
    "    \"Country: \" + df_corpus[\"countyName\"].fillna(\"\").astype(str) + \"; \"\n",
    "    \"City: \" + df_corpus[\"cityName\"].fillna(\"\").astype(str) + \"; \"\n",
    "    \"Hotel: \" + df_corpus[\"HotelName\"].fillna(\"\").astype(str) + \"; \"\n",
    "    \"Description: \" + df_corpus[\"Description\"].fillna(\"\").astype(str) + \"; \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- switch to bm25s ---\n",
    "# pip install \"bm25s[full]\"\n",
    "import bm25s\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# ---- load your data (same as before) ----\n",
    "# df_corpus: DataFrame with column \"combined\"\n",
    "# queries_df: CSV with column \"prompt\"\n",
    "queries_df = pd.read_csv(\"final_benchmark.csv\")\n",
    "\n",
    "# ---- tokenize & index corpus with bm25s ----\n",
    "# Using builtin tokenizer with English stopwords; remove stopwords=\"en\" if undesired\n",
    "corpus_texts = df_corpus[\"combined\"].astype(str).tolist()\n",
    "# tokenize once\n",
    "corpus_tokens = bm25s.tokenize(df_corpus[\"combined\"].astype(str).tolist(), stopwords=\"en\")\n",
    "retriever = bm25s.BM25()\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "query_texts = queries_df[\"query\"].astype(str).tolist()\n",
    "query_tokens = bm25s.tokenize(query_texts)  # list of token-id lists\n",
    "\n",
    "start_all = time.time()\n",
    "doc_ids, scores = retriever.retrieve(query_tokens, k=3)  # shapes: (N, 3) each\n",
    "elapsed_all = time.time() - start_all\n",
    "\n",
    "rows, latencies = [], []\n",
    "for i, prompt_text in enumerate(query_texts):\n",
    "    ids = doc_ids[i].tolist()\n",
    "    top_docs = df_corpus.iloc[ids][\"combined\"].astype(str).tolist()\n",
    "    while len(top_docs) < 3:\n",
    "        top_docs.append(\"\")\n",
    "    rec_text = f\"I have 3 recommendations: 1. {top_docs[0]} 2. {top_docs[1]} 3. {top_docs[2]}\"\n",
    "    rows.append({\"prompt\": prompt_text, \"recommendations\": rec_text})\n",
    "\n",
    "# If you still want per-query latencies, time in chunks or loop; otherwise:\n",
    "latencies = [elapsed_all / len(query_texts)] * len(query_texts)\n",
    "\n",
    "# ---- latency stats ----\n",
    "latencies = np.array(latencies, dtype=float)\n",
    "p50 = np.percentile(latencies, 50)\n",
    "p99 = np.percentile(latencies, 99)\n",
    "print(f\"Latency p50 (median): {p50:.4f} sec\")\n",
    "print(f\"Latency p99: {p99:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- save results ----\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df[\"latency_sec\"] = latencies  # optional: add latency column\n",
    "print(results_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"exp1_bm25s_desc_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def norm(ex):\n",
    "    def S(x):  # safe string\n",
    "        return \"\" if x is None else str(x)\n",
    "    def F(x):  # safe float\n",
    "        if x in (None, \"\"): return None\n",
    "        try: return float(x)\n",
    "        except: return None\n",
    "    def J(x):  # json -> string\n",
    "        if isinstance(x, (dict, list)):\n",
    "            return json.dumps(x, ensure_ascii=False)\n",
    "        return S(x)\n",
    "\n",
    "    return {\n",
    "        \"date\": S(ex.get(\"date\")),\n",
    "        \"rating\": F(ex.get(\"rating\")),\n",
    "        \"title\": S(ex.get(\"title\")),\n",
    "        \"text\": S(ex.get(\"text\")),\n",
    "        \"property_dict\": J(ex.get(\"property_dict\", {})),\n",
    "        \"Name\": S(ex.get(\"Name\")),\n",
    "        \"City\": S(ex.get(\"City\")),\n",
    "        \"County\": S(ex.get(\"County\")),\n",
    "    }\n",
    "\n",
    "schema = pa.schema([\n",
    "    (\"date\", pa.string()),\n",
    "    (\"rating\", pa.float64()),\n",
    "    (\"title\", pa.string()),\n",
    "    (\"text\", pa.string()),\n",
    "    (\"property_dict\", pa.string()),\n",
    "    (\"Name\", pa.string()),\n",
    "    (\"City\", pa.string()),\n",
    "    (\"County\", pa.string()),\n",
    "])\n",
    "\n",
    "stream = load_dataset(\"json\", data_files=\"HotelRec_V5.jsonl\", split=\"train\", streaming=True)\n",
    "\n",
    "out_path = \"HotelRec_V5.parquet\"\n",
    "writer = pq.ParquetWriter(out_path, schema)\n",
    "buf, B = [], 200_000\n",
    "\n",
    "for ex in tqdm(stream, desc=\"Normalizing + writing\"):\n",
    "    buf.append(norm(ex))\n",
    "    if len(buf) >= B:\n",
    "        writer.write_table(pa.Table.from_pandas(pd.DataFrame(buf), schema=schema, preserve_index=False))\n",
    "        buf.clear()\n",
    "\n",
    "if buf:\n",
    "    writer.write_table(pa.Table.from_pandas(pd.DataFrame(buf), schema=schema, preserve_index=False))\n",
    "writer.close()\n",
    "\n",
    "# Later: super fast loads\n",
    "# df = pd.read_parquet(\"HotelRec_V5.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0170a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"HotelRec.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee54de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build the 'combined' column on your existing df\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def _s(sr):\n",
    "    return (\n",
    "        sr.fillna(\"\")\n",
    "          .astype(str)\n",
    "          .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "\n",
    "# clean pieces\n",
    "name   = _s(df.get(\"Name\"))\n",
    "city   = _s(df.get(\"City\"))\n",
    "county = _s(df.get(\"County\"))          # your data uses 'County'\n",
    "title  = _s(df.get(\"title\"))\n",
    "text   = _s(df.get(\"text\")).str.slice(0, 2000)\n",
    "\n",
    "# rating: numeric -> string ('' if NA)\n",
    "rating_num = pd.to_numeric(df.get(\"rating\"), errors=\"coerce\")\n",
    "rating_str = rating_num.round(1).astype(\"Float64\").astype(str).replace(\"<NA>\", \"\")\n",
    "\n",
    "df[\"combined\"] = (\n",
    "    \"Hotel: \" + name +\n",
    "    \"; City: \" + city +\n",
    "    \"; Country: \" + county +\n",
    "    \"; Rating: \" + rating_str +\n",
    "    \". Title: \" + title +\n",
    "    \". Review: \" + text\n",
    ")\n",
    "\n",
    "# If you want a lighter object for retrieval:\n",
    "df_corpus = df[[\"combined\"]]   # or: df_corpus = pd.DataFrame({\"combined\": df[\"combined\"]})\n",
    "print(\"Corpus size:\", df_corpus.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) BM25S retrieval over the 'combined' column (your block)\n",
    "# pip install \"bm25s[full]\"\n",
    "import bm25s\n",
    "\n",
    "# Tokenize corpus (stopwords=\"en\" optional)\n",
    "corpus_texts  = df_corpus[\"combined\"].astype(str).tolist()\n",
    "corpus_tokens = bm25s.tokenize(corpus_texts, stopwords=\"en\")\n",
    "\n",
    "retriever = bm25s.BM25()\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "# Load queries. Supports either 'query' or 'prompt'\n",
    "queries_df = pd.read_csv(\"final_benchmark.csv\")\n",
    "if \"query\" in queries_df.columns:\n",
    "    query_texts = queries_df[\"query\"].astype(str).tolist()\n",
    "elif \"prompt\" in queries_df.columns:\n",
    "    query_texts = queries_df[\"prompt\"].astype(str).tolist()\n",
    "else:\n",
    "    raise ValueError(\"final_benchmark.csv must have a 'query' or 'prompt' column.\")\n",
    "\n",
    "query_tokens = bm25s.tokenize(query_texts)\n",
    "\n",
    "start_all = time.time()\n",
    "doc_ids, scores = retriever.retrieve(query_tokens, k=3)  # shapes: (N, 3)\n",
    "elapsed_all = time.time() - start_all\n",
    "avg_latency = elapsed_all / max(1, len(query_texts))\n",
    "\n",
    "# Build result rows\n",
    "rows = []\n",
    "for i, q in enumerate(query_texts):\n",
    "    ids = doc_ids[i].tolist()\n",
    "    top_docs = df_corpus.iloc[ids][\"combined\"].astype(str).tolist()\n",
    "    while len(top_docs) < 3:\n",
    "        top_docs.append(\"\")\n",
    "    rec_text = f\"I have 3 recommendations: 1. {top_docs[0]} 2. {top_docs[1]} 3. {top_docs[2]}\"\n",
    "    rows.append({\"prompt\": q, \"recommendations\": rec_text})\n",
    "\n",
    "# Latency stats (avg duplicated per-query; for true per-query, time inside loop)\n",
    "latencies = np.array([avg_latency] * len(rows), dtype=float)\n",
    "p50 = np.percentile(latencies, 50)\n",
    "p99 = np.percentile(latencies, 99)\n",
    "print(f\"Latency p50 (median): {p50:.4f} sec\")\n",
    "print(f\"Latency p99: {p99:.4f} sec\")\n",
    "\n",
    "# Save\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df[\"latency_sec\"] = latencies\n",
    "print(results_df.head(3))\n",
    "results_df.to_csv(\"exp1_bm25s_rev_results.csv\", index=False)\n",
    "print(\"[OK] Saved exp1_bm25s_rev_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09c55a",
   "metadata": {},
   "source": [
    "# dense retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e46aa",
   "metadata": {},
   "source": [
    "## Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_corpus = pd.read_csv(\"hotels.csv\", encoding=\"latin1\")\n",
    "df_corpus.columns = df_corpus.columns.str.strip()\n",
    "\n",
    "df_corpus[\"combined\"] = (\n",
    "    \"Country: \" + df_corpus[\"countyName\"].fillna(\"\").astype(str) + \"; \"\n",
    "    \"City: \" + df_corpus[\"cityName\"].fillna(\"\").astype(str) + \"; \"\n",
    "    \"Hotel: \" + df_corpus[\"HotelName\"].fillna(\"\").astype(str) + \"; \"\n",
    "    \"Description: \" + df_corpus[\"Description\"].fillna(\"\").astype(str) + \"; \"\n",
    ")\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# ---- load your data ----\n",
    "# df_corpus: DataFrame with column \"combined\"\n",
    "# queries_df: CSV with column \"query\"\n",
    "queries_df = pd.read_csv(\"final_benchmark.csv\")\n",
    "\n",
    "# ---- embed corpus with all-MiniLM-L6-v2 ----\n",
    "# model = SentenceTransformer(\"Alibaba-NLP/gte-multilingual-base\", trust_remote_code=True)\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", model_kwargs = {\"torch_dtype\" : torch.bfloat16},\n",
    "                             )\n",
    "corpus_texts = df_corpus[\"combined\"].astype(str).tolist()\n",
    "\n",
    "start_index = time.time()\n",
    "corpus_emb = model.encode_document(\n",
    "    corpus_texts, batch_size=128, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "# cosine via inner product on unit-normalized vectors\n",
    "index = faiss.IndexFlatIP(corpus_emb.shape[1])\n",
    "index.add(corpus_emb)\n",
    "index_build_secs = time.time() - start_index\n",
    "\n",
    "# ---- embed queries & retrieve top-3 ----\n",
    "query_texts = queries_df[\"query\"].astype(str).tolist()\n",
    "\n",
    "start_all = time.time()\n",
    "query_emb = model.encode_query(\n",
    "    query_texts, batch_size=256, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False\n",
    ").astype(\"float32\")\n",
    "\n",
    "scores, ids = index.search(query_emb, k=3)  # shapes: (N, 3)\n",
    "elapsed_all = time.time() - start_all\n",
    "\n",
    "# ---- build recommendations per query ----\n",
    "rows = []\n",
    "for i, q in enumerate(query_texts):\n",
    "    top_ids = ids[i].tolist()\n",
    "    top_docs = df_corpus.iloc[top_ids][\"combined\"].astype(str).tolist()\n",
    "    while len(top_docs) < 3:\n",
    "        top_docs.append(\"\")\n",
    "    rec_text = f\"I have 3 recommendations: 1. {top_docs[0]} 2. {top_docs[1]} 3. {top_docs[2]}\"\n",
    "    rows.append({\"prompt\": q, \"recommendations\": rec_text})\n",
    "\n",
    "# ---- latency stats (overall, + per-query avg like before) ----\n",
    "latencies = np.array([elapsed_all / max(len(query_texts), 1)] * len(query_texts), dtype=float)\n",
    "p50 = np.percentile(latencies, 50)\n",
    "p99 = np.percentile(latencies, 99)\n",
    "\n",
    "print(f\"Index build time: {index_build_secs:.3f} sec\")\n",
    "print(f\"Query batch time: {elapsed_all:.3f} sec for {len(query_texts)} queries\")\n",
    "print(f\"Latency p50 (median): {p50:.4f} sec\")\n",
    "print(f\"Latency p99: {p99:.4f} sec\")\n",
    "\n",
    "# Optional: results DataFrame\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df[\"latency_sec\"] = latencies  # optional: add latency column\n",
    "print(results_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(\"exp1_300m_desc_results_g.csv\", index=False)\n",
    "faiss.write_index(index, \"exp1_300m_desc_index_g.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"HotelRec_V5.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build the 'combined' column on your existing df\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def _s(sr):\n",
    "    return (\n",
    "        sr.fillna(\"\")\n",
    "          .astype(str)\n",
    "          .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "\n",
    "# clean pieces\n",
    "name   = _s(df.get(\"Name\"))\n",
    "city   = _s(df.get(\"City\"))\n",
    "county = _s(df.get(\"County\"))          # your data uses 'County'\n",
    "title  = _s(df.get(\"title\"))\n",
    "text   = _s(df.get(\"text\")).str.slice(0, 2000)\n",
    "\n",
    "# rating: numeric -> string ('' if NA)\n",
    "rating_num = pd.to_numeric(df.get(\"rating\"), errors=\"coerce\")\n",
    "rating_str = rating_num.round(1).astype(\"Float64\").astype(str).replace(\"<NA>\", \"\")\n",
    "\n",
    "df[\"combined\"] = (\n",
    "    \"Hotel: \" + name +\n",
    "    \"; City: \" + city +\n",
    "    \"; Country: \" + county +\n",
    "    \"; Rating: \" + rating_str +\n",
    "    \". Title: \" + title +\n",
    "    \". Review: \" + text\n",
    ")\n",
    "\n",
    "# If you want a lighter object for retrieval:\n",
    "df_corpus = df[[\"combined\"]]   # or: df_corpus = pd.DataFrame({\"combined\": df[\"combined\"]})\n",
    "print(\"Corpus size:\", df_corpus.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus.to_parquet(\"exp1_rev.parquet\", index=False)  # default engine=pyarrow if installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets pyarrow\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your parquet file as a Hugging Face Dataset\n",
    "dataset = load_dataset(\"parquet\", data_files=\"exp1_rev.parquet\")[\"train\"]\n",
    "\n",
    "# Inspect it\n",
    "print(dataset)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e68431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers faiss-cpu datasets tqdm\n",
    "\n",
    "import os, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "BATCH_SIZE = 1024\n",
    "EMB_DIM = 384                     # all-MiniLM-L6-v2 output dim\n",
    "MEMMAP_PATH = \"corpus_emb.f16.memmap\"   # stored as float16 to save disk, convert to f32 when adding to FAISS\n",
    "USE_FLOAT16_DISK = True\n",
    "\n",
    "# HNSW params (tune for speed/accuracy tradeoff)\n",
    "HNSW_M = 32                       # graph degree (16–64 is common)\n",
    "HNSW_EF_CONSTRUCTION = 200        # build-time accuracy/speed\n",
    "HNSW_EF_SEARCH = 128              # query-time accuracy/speed\n",
    "\n",
    "K = 3                             # top-k to retrieve\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "N = len(dataset)\n",
    "print(f\"Corpus size: {N:,}\")\n",
    "\n",
    "dtype_disk = np.float16 if USE_FLOAT16_DISK else np.float32\n",
    "embs_mm = np.memmap(MEMMAP_PATH, dtype=dtype_disk, mode=\"w+\", shape=(N, EMB_DIM))\n",
    "\n",
    "t0 = time.time()\n",
    "for start in tqdm(range(0, N, BATCH_SIZE), desc=\"Encoding corpus\"):\n",
    "    end = min(start + BATCH_SIZE, N)\n",
    "    texts = dataset[start:end][\"combined\"]  # pulls only this batch\n",
    "    batch_emb = model.encode(\n",
    "        texts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,  # unit norm -> cosine == dot == monotonic to L2\n",
    "        show_progress_bar=False\n",
    "    ).astype(\"float32\")            # model returns float32\n",
    "\n",
    "    # store to disk (optionally downcast)\n",
    "    if USE_FLOAT16_DISK:\n",
    "        embs_mm[start:end] = batch_emb.astype(\"float16\")\n",
    "    else:\n",
    "        embs_mm[start:end] = batch_emb\n",
    "embs_mm.flush()\n",
    "encode_secs = time.time() - t0\n",
    "print(f\"[OK] Encoded corpus in {encode_secs/60:.2f} min. Saved memmap: {MEMMAP_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build HNSW index incrementally (L2 on normalized vectors ≡ cosine ranking)\n",
    "# ----------------------------\n",
    "index = faiss.IndexHNSWFlat(EMB_DIM, HNSW_M)  # default metric: L2\n",
    "index.hnsw.efConstruction = HNSW_EF_CONSTRUCTION\n",
    "index.hnsw.efSearch = HNSW_EF_SEARCH\n",
    "\n",
    "# Add vectors shard-by-shard to avoid RAM spikes\n",
    "ADD_SHARD = 200_000\n",
    "t0 = time.time()\n",
    "for start in tqdm(range(0, N, ADD_SHARD), desc=\"Building HNSW (add)\"):\n",
    "    end = min(start + ADD_SHARD, N)\n",
    "    shard = np.array(embs_mm[start:end], dtype=\"float32\")  # FAISS expects float32\n",
    "    # already normalized above; if you skipped normalize_embeddings, do:\n",
    "    # faiss.normalize_L2(shard)\n",
    "    index.add(shard)\n",
    "build_secs = time.time() - t0\n",
    "print(f\"[OK] HNSW built in {build_secs/60:.2f} min. ntotal={index.ntotal:,}\")\n",
    "\n",
    "faiss.write_index(index, \"hnsw_cosine.faiss\")\n",
    "print(\"[OK] Saved index -> hnsw_cosine.faiss\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Load queries, encode, and search\n",
    "# ----------------------------\n",
    "queries_df = pd.read_csv(\"final_benchmark.csv\")\n",
    "if \"query\" in queries_df.columns:\n",
    "    query_texts = queries_df[\"query\"].astype(str).tolist()\n",
    "elif \"prompt\" in queries_df.columns:\n",
    "    query_texts = queries_df[\"prompt\"].astype(str).tolist()\n",
    "else:\n",
    "    raise ValueError(\"final_benchmark.csv must have a 'query' or 'prompt' column.\")\n",
    "\n",
    "tqdm.write(f\"Num queries: {len(query_texts):,}\")\n",
    "\n",
    "t0 = time.time()\n",
    "q_emb = model.encode(\n",
    "    query_texts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "scores, ids = index.search(q_emb, k=K)\n",
    "elapsed_all = time.time() - t0\n",
    "avg_latency = elapsed_all / max(1, len(query_texts))\n",
    "p50 = float(np.percentile([avg_latency] * len(query_texts), 50))\n",
    "p99 = float(np.percentile([avg_latency] * len(query_texts), 99))\n",
    "print(f\"Query batch time: {elapsed_all:.3f} sec for {len(query_texts)} queries\")\n",
    "print(f\"Latency P50: {p50:.4f} s | P99: {p99:.4f} s (per-query avg proxy)\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Fetch top-k documents efficiently & build results\n",
    "# ----------------------------\n",
    "# Gather unique corpus ids once, fetch in one shot via select, then map back\n",
    "unique_ids = sorted(set(int(x) for row in ids for x in row if x >= 0))\n",
    "id_to_pos = {cid: i for i, cid in enumerate(unique_ids)}\n",
    "subset = dataset.select(unique_ids)  # keeps order of unique_ids\n",
    "subset_texts = subset[\"combined\"]\n",
    "\n",
    "def get_doc(doc_id):\n",
    "    return subset_texts[id_to_pos[int(doc_id)]]\n",
    "\n",
    "rows = []\n",
    "for i, q in enumerate(query_texts):\n",
    "    t_ids = ids[i].tolist()\n",
    "    t_docs = [get_doc(did) if did >= 0 else \"\" for did in t_ids]\n",
    "    while len(t_docs) < K:\n",
    "        t_docs.append(\"\")\n",
    "    rec_text = f\"I have {K} recommendations: \" + \" \".join(\n",
    "        [f\"{j+1}. {t_docs[j]}\" for j in range(K)]\n",
    "    )\n",
    "    rows.append({\"prompt\": q, \"recommendations\": rec_text})\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df[\"latency_sec\"] = avg_latency\n",
    "print(results_df.head(3))\n",
    "results_df.to_csv(\"exp1_22m_hnsw_results.csv\", index=False)\n",
    "print(\"[OK] Saved -> exp1_22m_hnsw_results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e7b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

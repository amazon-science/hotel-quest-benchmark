{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1eede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers faiss-cpu datasets tqdm transformers accelerate pyarrow pandas\n",
    "\n",
    "import os, time, math, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "PARQUET_PATH = \"exp1_rev.parquet\"          # your corpus parquet\n",
    "FAISS_INDEX_PATH = \"exp1_300m_desc_index_g.faiss\"        # your saved FAISS index\n",
    "QUERIES_CSV = \"final_benchmark.csv\"        # csv with a 'query' or 'prompt' column\n",
    "OUTPUT_CSV = \"exp1_300m_rerank600m_desc_results.csv\"\n",
    "\n",
    "MODEL_NAME = \"google/embeddinggemma-300m\"  # encoder model you used for the index\n",
    "BATCH_SIZE = 128\n",
    "EMB_DIM = 768\n",
    "USE_FLOAT16_DISK = True\n",
    "\n",
    "# HNSW params (info only; already baked into your saved index)\n",
    "HNSW_M = 32\n",
    "HNSW_EF_CONSTRUCTION = 200\n",
    "HNSW_EF_SEARCH = 128\n",
    "\n",
    "K = 100              # retrieve top-K from FAISS\n",
    "RERANK_TOP = 3       # keep only top-3 after reranking\n",
    "RERANK_BS = 8       # reranker batch size\n",
    "RERANKER_NAME = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "# If your parquet has a different text field name, list options here (first match wins)\n",
    "DOC_FIELD_CANDIDATES = [\"combined\", \"text\", \"document\", \"content\", \"passage\", \"body\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def pick_doc_field(hfds):\n",
    "    cols = set(hfds.column_names)\n",
    "    for c in DOC_FIELD_CANDIDATES:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    raise ValueError(f\"None of {DOC_FIELD_CANDIDATES} found in dataset columns: {sorted(cols)}\")\n",
    "\n",
    "def load_queries(path):\n",
    "    df = pd.read_csv(path)\n",
    "    if \"query\" in df.columns:\n",
    "        return df[\"query\"].astype(str).tolist(), \"query\"\n",
    "    if \"prompt\" in df.columns:\n",
    "        return df[\"prompt\"].astype(str).tolist(), \"prompt\"\n",
    "    raise ValueError(f\"{path} must contain a 'query' or 'prompt' column.\")\n",
    "\n",
    "def ensure_float32(x):\n",
    "    return x.astype(\"float32\") if x.dtype != np.float32 else x\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load corpus + queries + index + encoder\n",
    "# ----------------------------\n",
    "print(\"[1/5] Loading corpus parquet…\")\n",
    "dataset = load_dataset(\"parquet\", data_files=PARQUET_PATH)[\"train\"]\n",
    "DOC_FIELD = pick_doc_field(dataset)\n",
    "print(f\"     Using DOC_FIELD='{DOC_FIELD}'\")\n",
    "\n",
    "print(\"[2/5] Loading queries…\")\n",
    "query_texts, query_col = load_queries(QUERIES_CSV)\n",
    "print(f\"     Num queries: {len(query_texts):,}\")\n",
    "\n",
    "print(\"[3/5] Loading FAISS index…\")\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "print(f\"     [OK] Loaded FAISS index with ntotal={index.ntotal:,}\")\n",
    "\n",
    "print(\"[4/5] Loading encoder model for queries…\")\n",
    "device_enc = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc_model = SentenceTransformer(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")\n",
    "# Note: SentenceTransformer handles device internally; no .to(device) needed.\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Encode queries and search FAISS\n",
    "# ----------------------------\n",
    "print(\"[5/5] Encoding & retrieving…\")\n",
    "t0 = time.time()\n",
    "q_emb = enc_model.encode_query(\n",
    "    query_texts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "q_emb = ensure_float32(np.array(q_emb))\n",
    "scores, ids = index.search(q_emb, k=K)\n",
    "elapsed_all = time.time() - t0\n",
    "avg_latency = elapsed_all / max(1, len(query_texts))\n",
    "print(f\"     Retrieval time: {elapsed_all:.3f}s total | ~{avg_latency:.4f}s/query\")\n",
    "print(\"[*] Fetching top-K documents for all queries…\")\n",
    "unique_ids = sorted(set(int(x) for row in ids for x in row if x >= 0))\n",
    "id_to_pos = {cid: i for i, cid in enumerate(unique_ids)}\n",
    "subset = dataset.select(unique_ids)  # keeps order of unique_ids\n",
    "subset_texts = subset[DOC_FIELD]\n",
    "\n",
    "def get_doc(doc_id: int) -> str:\n",
    "    return subset_texts[id_to_pos[int(doc_id)]]\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Reranker (Qwen/Qwen3-Reranker-0.6B)\n",
    "# ----------------------------\n",
    "print(\"[*] Loading reranker…\")\n",
    "device_rank = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_NAME, padding_side='left')\n",
    "ranker_model = AutoModelForCausalLM.from_pretrained(\n",
    "    RERANKER_NAME,\n",
    "    # Uncomment if you have Flash-Attn 2 installed and want extra speed:\n",
    "    # torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ").to(device_rank).eval()\n",
    "\n",
    "token_false_id = ranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id  = ranker_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "if token_false_id == ranker_tokenizer.unk_token_id or token_true_id == ranker_tokenizer.unk_token_id:\n",
    "    raise RuntimeError(\"Reranker tokenizer is missing 'yes'/'no' tokens.\")\n",
    "\n",
    "max_length = 8192\n",
    "prefix = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "    \"Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    ")\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = ranker_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = ranker_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def _process_inputs(pairs):\n",
    "    inputs = ranker_tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation='longest_first',\n",
    "        return_attention_mask=True,\n",
    "        max_length=max_length - len(prefix_tokens) - len(suffix_tokens),\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    # wrap with prefix/suffix then pad (left-padding already set)\n",
    "    for i, ids_list in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ids_list + suffix_tokens\n",
    "    inputs = ranker_tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(device_rank)\n",
    "    return inputs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _compute_yes_scores(inputs):\n",
    "    logits = ranker_model(**inputs).logits[:, -1, :]     # last position\n",
    "    yes = logits[:, token_true_id]\n",
    "    no  = logits[:, token_false_id]\n",
    "    two = torch.stack([no, yes], dim=1)\n",
    "    return torch.nn.functional.log_softmax(two, dim=1)[:, 1].exp().tolist()\n",
    "\n",
    "def rerank_query(query: str, docs: list[str], task: str = None, top_n: int = 3) -> tuple[list[str], list[float], list[int]]:\n",
    "    \"\"\"Return top_n docs, scores, and original indices (highest score first).\"\"\"\n",
    "    all_scores = []\n",
    "    for start in range(0, len(docs), RERANK_BS):\n",
    "        chunk = docs[start:start+RERANK_BS]\n",
    "        pairs = [format_instruction(task, query, d) for d in chunk]\n",
    "        inputs = _process_inputs(pairs)\n",
    "        scores = _compute_yes_scores(inputs)\n",
    "        all_scores.extend(scores)\n",
    "    order = np.argsort(all_scores)[::-1][:top_n]\n",
    "    return [docs[i] for i in order], [all_scores[i] for i in order], order.tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Build results with top-3 per query\n",
    "# ----------------------------\n",
    "\n",
    "print(\"[*] Reranking to top-3 per query…\")\n",
    "task = \"Given a user's query, retrieve relevant passages that answer the all the query's requirements\"\n",
    "rows = []\n",
    "\n",
    "rerank_latencies = []   # per-query rerank latency (seconds)\n",
    "\n",
    "for i, q in enumerate(tqdm(query_texts, desc=\"Reranking\")):\n",
    "    # gather FAISS top-K docs for query i\n",
    "    q_ids = [int(d) for d in ids[i] if d >= 0]\n",
    "    q_docs = [get_doc(d) for d in q_ids]\n",
    "\n",
    "    # time the rerank for this query\n",
    "    t0 = time.perf_counter()\n",
    "    top_docs, top_scores, order = rerank_query(q, q_docs, task=task, top_n=RERANK_TOP)\n",
    "    dt = time.perf_counter() - t0\n",
    "    rerank_latencies.append(dt)\n",
    "\n",
    "    # map back to original corpus ids\n",
    "    top_doc_ids = [q_ids[j] for j in order]\n",
    "\n",
    "    # human-readable string\n",
    "    rec_text = \"I have 3 recommendations: \" + \" \".join([f\"{j+1}. {top_docs[j]}\" for j in range(len(top_docs))])\n",
    "\n",
    "    rows.append({\n",
    "        \"prompt\": q,\n",
    "        \"recommendations\": rec_text,\n",
    "        \"doc_ids_top3\": \"|\".join(str(x) for x in top_doc_ids),\n",
    "        \"scores_top3\": \"|\".join(f\"{s:.4f}\" for s in top_scores),\n",
    "    })\n",
    "\n",
    "# aggregate rerank timing\n",
    "rerank_total = float(np.sum(rerank_latencies))\n",
    "rerank_avg   = rerank_total / max(1, len(rerank_latencies))\n",
    "rerank_p50   = float(np.percentile(rerank_latencies, 50))\n",
    "rerank_p99   = float(np.percentile(rerank_latencies, 99))\n",
    "\n",
    "print(f\"     Rerank time: {rerank_total:.3f}s total | ~{rerank_avg:.4f}s/query | \"\n",
    "      f\"P50={rerank_p50:.4f}s | P99={rerank_p99:.4f}s\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save CSV (+ P50/P99)\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# If you have per-query retrieval latencies collected as `retrieval_latencies`,\n",
    "# you can compute their P50/P99 similarly. If not, we’ll keep using your avg_latency.\n",
    "results_df[\"retrieval_latency_sec_per_query\"] = avg_latency\n",
    "results_df[\"rerank_latency_sec_per_query\"]    = rerank_avg\n",
    "results_df[\"rerank_latency_p50_sec\"]          = rerank_p50\n",
    "results_df[\"rerank_latency_p99_sec\"]          = rerank_p99\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(results_df.head(3))\n",
    "\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Saved -> {OUTPUT_CSV}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_PATH = \"exp1_rev.parquet\"          # your corpus parquet\n",
    "FAISS_INDEX_PATH = \"300m_rev.faiss\"        # your saved FAISS index\n",
    "QUERIES_CSV = \"final_benchmark.csv\"        # csv with a 'query' or 'prompt' column\n",
    "OUTPUT_CSV = \"exp1_300m_rerank4B_rev_results.csv\"\n",
    "\n",
    "MODEL_NAME = \"google/embeddinggemma-300m\"  # encoder model you used for the index\n",
    "BATCH_SIZE = 128\n",
    "EMB_DIM = 768\n",
    "USE_FLOAT16_DISK = True\n",
    "\n",
    "# HNSW params (info only; already baked into your saved index)\n",
    "HNSW_M = 32\n",
    "HNSW_EF_CONSTRUCTION = 200\n",
    "HNSW_EF_SEARCH = 128\n",
    "\n",
    "K = 100              # retrieve top-K from FAISS\n",
    "RERANK_TOP = 3       # keep only top-3 after reranking\n",
    "RERANK_BS = 4       # reranker batch size\n",
    "RERANKER_NAME = \"Qwen/Qwen3-Reranker-4B\"\n",
    "\n",
    "\n",
    "print(\"[*] Loading reranker…\")\n",
    "device_rank = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_NAME, padding_side='left')\n",
    "ranker_model = AutoModelForCausalLM.from_pretrained(\n",
    "    RERANKER_NAME,\n",
    "    # Uncomment if you have Flash-Attn 2 installed and want extra speed:\n",
    "    # torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    ").to(device_rank).eval()\n",
    "\n",
    "token_false_id = ranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id  = ranker_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "if token_false_id == ranker_tokenizer.unk_token_id or token_true_id == ranker_tokenizer.unk_token_id:\n",
    "    raise RuntimeError(\"Reranker tokenizer is missing 'yes'/'no' tokens.\")\n",
    "\n",
    "max_length = 8192\n",
    "prefix = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "    \"Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    ")\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = ranker_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = ranker_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    return f\"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def _process_inputs(pairs):\n",
    "    inputs = ranker_tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation='longest_first',\n",
    "        return_attention_mask=True,\n",
    "        max_length=max_length - len(prefix_tokens) - len(suffix_tokens),\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    # wrap with prefix/suffix then pad (left-padding already set)\n",
    "    for i, ids_list in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ids_list + suffix_tokens\n",
    "    inputs = ranker_tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(device_rank)\n",
    "    return inputs\n",
    "\n",
    "@torch.inference_mode()\n",
    "def _compute_yes_scores(inputs):\n",
    "    logits = ranker_model(**inputs).logits[:, -1, :]     # last position\n",
    "    yes = logits[:, token_true_id]\n",
    "    no  = logits[:, token_false_id]\n",
    "    two = torch.stack([no, yes], dim=1)\n",
    "    return torch.nn.functional.log_softmax(two, dim=1)[:, 1].exp().tolist()\n",
    "\n",
    "def rerank_query(query: str, docs: list[str], task: str = None, top_n: int = 3) -> tuple[list[str], list[float], list[int]]:\n",
    "    \"\"\"Return top_n docs, scores, and original indices (highest score first).\"\"\"\n",
    "    all_scores = []\n",
    "    for start in range(0, len(docs), RERANK_BS):\n",
    "        chunk = docs[start:start+RERANK_BS]\n",
    "        pairs = [format_instruction(task, query, d) for d in chunk]\n",
    "        inputs = _process_inputs(pairs)\n",
    "        scores = _compute_yes_scores(inputs)\n",
    "        all_scores.extend(scores)\n",
    "    order = np.argsort(all_scores)[::-1][:top_n]\n",
    "    return [docs[i] for i in order], [all_scores[i] for i in order], order.tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Build results with top-3 per query\n",
    "# ----------------------------\n",
    "\n",
    "print(\"[*] Reranking to top-3 per query…\")\n",
    "task = \"Given a user's query, retrieve relevant passages that answer the all the query's requirements\"\n",
    "rows = []\n",
    "\n",
    "rerank_latencies = []   # per-query rerank latency (seconds)\n",
    "\n",
    "for i, q in enumerate(tqdm(query_texts, desc=\"Reranking\")):\n",
    "    # gather FAISS top-K docs for query i\n",
    "    q_ids = [int(d) for d in ids[i] if d >= 0]\n",
    "    q_docs = [get_doc(d) for d in q_ids]\n",
    "\n",
    "    # time the rerank for this query\n",
    "    t0 = time.perf_counter()\n",
    "    top_docs, top_scores, order = rerank_query(q, q_docs, task=task, top_n=RERANK_TOP)\n",
    "    dt = time.perf_counter() - t0\n",
    "    rerank_latencies.append(dt)\n",
    "\n",
    "    # map back to original corpus ids\n",
    "    top_doc_ids = [q_ids[j] for j in order]\n",
    "\n",
    "    # human-readable string\n",
    "    rec_text = \"I have 3 recommendations: \" + \" \".join([f\"{j+1}. {top_docs[j]}\" for j in range(len(top_docs))])\n",
    "\n",
    "    rows.append({\n",
    "        \"prompt\": q,\n",
    "        \"recommendations\": rec_text,\n",
    "        \"doc_ids_top3\": \"|\".join(str(x) for x in top_doc_ids),\n",
    "        \"scores_top3\": \"|\".join(f\"{s:.4f}\" for s in top_scores),\n",
    "    })\n",
    "\n",
    "# aggregate rerank timing\n",
    "rerank_total = float(np.sum(rerank_latencies))\n",
    "rerank_avg   = rerank_total / max(1, len(rerank_latencies))\n",
    "rerank_p50   = float(np.percentile(rerank_latencies, 50))\n",
    "rerank_p99   = float(np.percentile(rerank_latencies, 99))\n",
    "\n",
    "print(f\"     Rerank time: {rerank_total:.3f}s total | ~{rerank_avg:.4f}s/query | \"\n",
    "      f\"P50={rerank_p50:.4f}s | P99={rerank_p99:.4f}s\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Save CSV (+ P50/P99)\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# If you have per-query retrieval latencies collected as `retrieval_latencies`,\n",
    "# you can compute their P50/P99 similarly. If not, we’ll keep using your avg_latency.\n",
    "results_df[\"retrieval_latency_sec_per_query\"] = avg_latency\n",
    "results_df[\"rerank_latency_sec_per_query\"]    = rerank_avg\n",
    "results_df[\"rerank_latency_p50_sec\"]          = rerank_p50\n",
    "results_df[\"rerank_latency_p99_sec\"]          = rerank_p99\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(results_df.head(3))\n",
    "\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Saved -> {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83962c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers faiss-cpu datasets tqdm transformers accelerate pyarrow pandas\n",
    "\n",
    "import os, time, math, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Load reviews corpus from Parquet (HF Dataset)\n",
    "# ----------------------------\n",
    "PARQUET_PATH = \"exp1_rev.parquet\"          # <-- your reviews parquet\n",
    "FAISS_INDEX_PATH = \"300m_rev.faiss\"        # <-- FAISS built over the SAME row order as this parquet\n",
    "QUERIES_CSV      = \"final_benchmark.csv\"   # csv with 'query' or 'prompt'\n",
    "OUTPUT_CSV       = \"exp1_300m_rerank4b_reviews_results.csv\"\n",
    "\n",
    "MODEL_NAME    = \"google/embeddinggemma-300m\"\n",
    "BATCH_SIZE    = 128\n",
    "EMB_DIM       = 768\n",
    "K             = 100\n",
    "RERANK_TOP    = 3\n",
    "RERANK_BS     = 8\n",
    "RERANKER_NAME = \"Qwen/Qwen3-Reranker-4B\"\n",
    "\n",
    "# If your parquet has a different text field name, list options here (first match wins)\n",
    "DOC_FIELD_CANDIDATES = [\n",
    "    \"combined\", \"text\", \"document\", \"content\", \"passage\", \"body\",\n",
    "    # common review fields\n",
    "    \"review_text\", \"review\", \"review_body\", \"reviewContent\"\n",
    "]\n",
    "\n",
    "def load_reviews_dataset(path: str):\n",
    "    ds = load_dataset(\"parquet\", data_files=path)[\"train\"]\n",
    "    return ds\n",
    "\n",
    "def pick_doc_field(hfds) -> str:\n",
    "    cols = set(hfds.column_names)\n",
    "    for c in DOC_FIELD_CANDIDATES:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    # Fallback: auto-compose a combined text from common review columns if present\n",
    "    review_parts = [c for c in [\n",
    "        \"review_title\", \"review_positive\", \"review_negative\", \"text\",\n",
    "        \"review_text\", \"review_body\", \"pros\", \"cons\"\n",
    "    ] if c in cols]\n",
    "    if review_parts:\n",
    "        return \"__compose_reviews__\"\n",
    "    raise ValueError(f\"None of {DOC_FIELD_CANDIDATES} found and no typical review fields \"\n",
    "                     f\"in dataset columns: {sorted(cols)}\")\n",
    "\n",
    "def _is_nan(x):\n",
    "    return isinstance(x, float) and math.isnan(x)\n",
    "\n",
    "def _val(x):\n",
    "    return \"\" if x is None or _is_nan(x) else str(x)\n",
    "\n",
    "dataset = load_reviews_dataset(PARQUET_PATH)\n",
    "DOC_FIELD = pick_doc_field(dataset)\n",
    "N_CORPUS = len(dataset)\n",
    "print(f\"[corpus] Using HF Dataset from '{PARQUET_PATH}', n_docs={N_CORPUS:,}\")\n",
    "print(f\"[corpus] Using DOC_FIELD='{DOC_FIELD}'\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Queries\n",
    "# ----------------------------\n",
    "def load_queries(path):\n",
    "    dfq = pd.read_csv(path)\n",
    "    if \"query\" in dfq.columns:\n",
    "        return dfq[\"query\"].astype(str).tolist(), \"query\"\n",
    "    if \"prompt\" in dfq.columns:\n",
    "        return dfq[\"prompt\"].astype(str).tolist(), \"prompt\"\n",
    "    raise ValueError(f\"{path} must contain a 'query' or 'prompt' column.\")\n",
    "\n",
    "print(\"[1/4] Loading queries…\")\n",
    "query_texts, query_col = load_queries(QUERIES_CSV)\n",
    "print(f\"     Num queries: {len(query_texts):,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) FAISS index + encoder\n",
    "# ----------------------------\n",
    "print(\"[2/4] Loading FAISS index…\")\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "print(f\"     [OK] Loaded FAISS index with ntotal={index.ntotal:,}\")\n",
    "if index.ntotal != N_CORPUS:\n",
    "    print(f\"[WARN] FAISS index ntotal ({index.ntotal}) != dataset rows ({N_CORPUS}). \"\n",
    "          f\"Make sure the index and Dataset row order match!\", file=sys.stderr)\n",
    "\n",
    "def ensure_float32(x):\n",
    "    return x.astype(\"float32\") if x.dtype != np.float32 else x\n",
    "\n",
    "print(\"[3/4] Loading encoder model for queries…\")\n",
    "enc_model = SentenceTransformer(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() else torch.float32},\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Encode queries & search FAISS\n",
    "# ----------------------------\n",
    "print(\"[4/4] Encoding & retrieving…\")\n",
    "t0 = time.time()\n",
    "q_emb = enc_model.encode_query(\n",
    "    query_texts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "q_emb = ensure_float32(np.array(q_emb))\n",
    "scores, ids = index.search(q_emb, k=K)\n",
    "\n",
    "elapsed_all = time.time() - t0\n",
    "avg_latency = elapsed_all / max(1, len(query_texts))\n",
    "print(f\"     Retrieval time: {elapsed_all:.3f}s total | ~{avg_latency:.4f}s/query\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Doc accessor (HF Dataset)\n",
    "# ----------------------------\n",
    "# If DOC_FIELD == \"__compose_reviews__\", we build a 'combined' string on the fly\n",
    "compose_fields = [c for c in [\"review_title\", \"review_positive\", \"review_negative\", \"text\",\n",
    "                              \"review_text\", \"review_body\", \"pros\", \"cons\"]\n",
    "                  if c in set(dataset.column_names)]\n",
    "\n",
    "def get_doc(doc_id: int) -> str:\n",
    "    if doc_id < 0 or doc_id >= N_CORPUS:\n",
    "        return \"\"\n",
    "    row = dataset[int(doc_id)]\n",
    "    if DOC_FIELD == \"__compose_reviews__\":\n",
    "        parts = []\n",
    "        if \"review_title\" in compose_fields and row.get(\"review_title\", None):\n",
    "            parts.append(f\"Title: {_val(row.get('review_title'))}\")\n",
    "        if \"text\" in compose_fields and row.get(\"text\", None):\n",
    "            parts.append(_val(row.get(\"text\")))\n",
    "        if \"review_text\" in compose_fields and row.get(\"review_text\", None):\n",
    "            parts.append(_val(row.get(\"review_text\")))\n",
    "        if \"review_body\" in compose_fields and row.get(\"review_body\", None):\n",
    "            parts.append(_val(row.get(\"review_body\")))\n",
    "        if \"review_positive\" in compose_fields and row.get(\"review_positive\", None):\n",
    "            parts.append(f\"Pros: {_val(row.get('review_positive'))}\")\n",
    "        if \"review_negative\" in compose_fields and row.get(\"review_negative\", None):\n",
    "            parts.append(f\"Cons: {_val(row.get('review_negative'))}\")\n",
    "        if \"pros\" in compose_fields and row.get(\"pros\", None):\n",
    "            parts.append(f\"Pros: {_val(row.get('pros'))}\")\n",
    "        if \"cons\" in compose_fields and row.get(\"cons\", None):\n",
    "            parts.append(f\"Cons: {_val(row.get('cons'))}\")\n",
    "        return \" \".join(p for p in parts if p)\n",
    "    # simple single-field case\n",
    "    return _val(row.get(DOC_FIELD, \"\"))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Reranker (Qwen3-Reranker-4B) setup + helpers\n",
    "# ----------------------------\n",
    "_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_rerank_model_kwargs = {}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(RERANKER_NAME, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(RERANKER_NAME, **_rerank_model_kwargs).to(_device).eval()\n",
    "\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id  = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "\n",
    "max_length = getattr(model.config, \"max_position_embeddings\", 8192)\n",
    "prefix = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"Judge whether the Document meets the requirements based on the Query and the Instruct provided. \"\n",
    "    'Note that the answer can only be \"yes\" or \"no\".'\n",
    "    \"<|im_end|>\\n<|im_start|>user\\n\"\n",
    ")\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "def format_instruction(instruction: str, query: str, doc: str) -> str:\n",
    "    if instruction is None:\n",
    "        instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "    return \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "        instruction=instruction, query=query, doc=doc\n",
    "    )\n",
    "\n",
    "def _process_inputs(pairs: list[str]):\n",
    "    avail_len = max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    avail_len = int(max(16, avail_len))\n",
    "    inputs = tokenizer(\n",
    "        pairs,\n",
    "        padding=False,\n",
    "        truncation=\"longest_first\",\n",
    "        return_attention_mask=False,\n",
    "        max_length=avail_len,\n",
    "    )\n",
    "    for i, ids_ in enumerate(inputs[\"input_ids\"]):\n",
    "        inputs[\"input_ids\"][i] = prefix_tokens + ids_ + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for k in inputs:\n",
    "        inputs[k] = inputs[k].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def _compute_yes_scores(inputs) -> list[float]:\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    true_vector  = batch_scores[:, token_true_id]\n",
    "    two_class = torch.stack([false_vector, true_vector], dim=1)\n",
    "    two_class = torch.nn.functional.log_softmax(two_class, dim=1)\n",
    "    return two_class[:, 1].exp().tolist()  # P(\"yes\")\n",
    "\n",
    "def rerank_query(query: str, docs: list[str], task: str = None, top_n: int = 3):\n",
    "    if not docs:\n",
    "        return [], [], []\n",
    "    all_scores = []\n",
    "    for start in range(0, len(docs), RERANK_BS):\n",
    "        chunk = docs[start:start+RERANK_BS]\n",
    "        pairs = [format_instruction(task, query, d) for d in chunk]\n",
    "        inputs = _process_inputs(pairs)\n",
    "        scores = _compute_yes_scores(inputs)\n",
    "        all_scores.extend(scores)\n",
    "    order = np.argsort(all_scores)[::-1][:top_n]\n",
    "    return [docs[i] for i in order], [all_scores[i] for i in order], order.tolist()\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Rerank top-K per query -> top-3\n",
    "# ----------------------------\n",
    "print(\"[*] Reranking to top-3 per query…\")\n",
    "task = \"Given a user's query, retrieve relevant review passages that answer all the query's requirements\"\n",
    "rows = []\n",
    "rerank_latencies = []\n",
    "\n",
    "for i, q in enumerate(tqdm(query_texts, desc=\"Reranking\")):\n",
    "    q_ids = [int(d) for d in ids[i] if d >= 0 and d < N_CORPUS]\n",
    "    q_docs = [get_doc(d) for d in q_ids]\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    top_docs, top_scores, order = rerank_query(q, q_docs, task=task, top_n=RERANK_TOP)\n",
    "    dt = time.perf_counter() - t0\n",
    "    rerank_latencies.append(dt)\n",
    "\n",
    "    top_doc_ids = [q_ids[j] for j in order]\n",
    "\n",
    "    rec_text = \"Top-3 relevant review snippets: \" + \" \".join(\n",
    "        [f\"{j+1}. {top_docs[j]}\" for j in range(len(top_docs))]\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        \"prompt\": q,\n",
    "        \"recommendations\": rec_text,\n",
    "        \"doc_ids_top3\": \"|\".join(str(x) for x in top_doc_ids),\n",
    "        \"scores_top3\": \"|\".join(f\"{s:.4f}\" for s in top_scores),\n",
    "    })\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Latency stats + save\n",
    "# ----------------------------\n",
    "rerank_total = float(np.sum(rerank_latencies))\n",
    "rerank_avg   = rerank_total / max(1, len(rerank_latencies))\n",
    "rerank_p50   = float(np.percentile(rerank_latencies, 50))\n",
    "rerank_p99   = float(np.percentile(rerank_latencies, 99))\n",
    "\n",
    "print(f\"     Rerank time: {rerank_total:.3f}s total | ~{rerank_avg:.4f}s/query | \"\n",
    "      f\"P50={rerank_p50:.4f}s | P99={rerank_p99:.4f}s\")\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "results_df[\"retrieval_latency_sec_per_query\"] = avg_latency\n",
    "results_df[\"rerank_latency_sec_per_query\"]    = rerank_avg\n",
    "results_df[\"rerank_latency_p50_sec\"]          = rerank_p50\n",
    "results_df[\"rerank_latency_p99_sec\"]          = rerank_p99\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(results_df.head(3))\n",
    "\n",
    "results_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n[OK] Saved -> {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

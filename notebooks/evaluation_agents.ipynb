{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://127.0.0.1:6006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44965b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "client = px.Client()\n",
    "\n",
    "df = pd.read_csv(\"hotels.csv\", encoding=\"latin1\") # description data\n",
    "exp = pd.read_csv(\"final_benchmark.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2592c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional, Callable\n",
    "\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "def build_implicit_lookup(\n",
    "    exp_df: pd.DataFrame,\n",
    "    query_col: str = \"query\",\n",
    "    implicit_col: str = 'Clarification Ground Truth'\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create a mapping: normalized query text -> implicit explanation.\n",
    "    Drops NaNs and duplicates (keeps the first).\n",
    "    \"\"\"\n",
    "    if query_col not in exp_df.columns:\n",
    "        raise KeyError(f\"'{query_col}' not found in exp_df columns: {list(exp_df.columns)}\")\n",
    "    if implicit_col not in exp_df.columns:\n",
    "        raise KeyError(f\"'{implicit_col}' not found in exp_df columns: {list(exp_df.columns)}\")\n",
    "\n",
    "    tmp = exp_df[[query_col, implicit_col]].dropna()\n",
    "    # Keep first occurrence per normalized query\n",
    "    tmp[\"_key\"] = tmp[query_col].astype(str).map(_normalize)\n",
    "    tmp = tmp.drop_duplicates(subset=[\"_key\"], keep=\"first\")\n",
    "    return dict(zip(tmp[\"_key\"], tmp[implicit_col].astype(str)))\n",
    "\n",
    "\n",
    "\n",
    "_CITATION_RE = re.compile(\n",
    "    r\"\\[(Hotel review|Hotel description|Web)(?::\\s*([^\\]]*?))?\\]\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def extract_citations(text: str, return_unique: bool = True) -> Tuple[List[dict], List[dict]]:\n",
    "    \"\"\"\n",
    "    Extract only citations where the value (hotel name, URL, etc.) is not empty.\n",
    "    Returns (all_hits, unique_hits).\n",
    "    \"\"\"\n",
    "    type_map = {\n",
    "        \"hotel review\": \"review\",\n",
    "        \"hotel description\": \"description\",\n",
    "        \"web\": \"web\",\n",
    "    }\n",
    "\n",
    "    all_hits = []\n",
    "    for m in _CITATION_RE.finditer(text):\n",
    "        value = m.group(2).strip() if m.group(2) else None\n",
    "        if not value:\n",
    "            continue\n",
    "        kind_raw = m.group(1).strip().lower()\n",
    "        kind = type_map.get(kind_raw, kind_raw)\n",
    "        all_hits.append({\n",
    "            \"raw\": m.group(0),\n",
    "            \"type\": kind,\n",
    "            \"value\": value,\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "        })\n",
    "\n",
    "    if not return_unique:\n",
    "        return all_hits, []\n",
    "\n",
    "    keyseq = [(hit[\"type\"], hit[\"value\"].lower()) for hit in all_hits]\n",
    "    counts = Counter(keyseq)\n",
    "\n",
    "    first_seen_value = {}\n",
    "    for hit in all_hits:\n",
    "        k = (hit[\"type\"], hit[\"value\"].lower())\n",
    "        first_seen_value.setdefault(k, hit[\"value\"])\n",
    "\n",
    "    unique_hits = [\n",
    "        {\"type\": t, \"value\": first_seen_value[(t, v)], \"count\": c}\n",
    "        for (t, v), c in counts.items()\n",
    "    ]\n",
    "    unique_hits.sort(key=lambda d: (d[\"type\"], d[\"value\"]))\n",
    "    return all_hits, unique_hits\n",
    "\n",
    "\n",
    "def get_descriptions_from_df(\n",
    "    unique_hits: List[Dict],\n",
    "    df: pd.DataFrame,\n",
    "    name_col: str = \"Name\",\n",
    "    desc_col: str = \"description\",\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Return list of (hotel_name, description) for names found in unique_hits (type='description').\n",
    "    \"\"\"\n",
    "    names = [hit[\"value\"] for hit in unique_hits if hit[\"type\"] == \"description\"]\n",
    "    if not names:\n",
    "        return []\n",
    "    mask = df[name_col].isin(names)\n",
    "    rows = df.loc[mask, [name_col, desc_col]].dropna().drop_duplicates()\n",
    "    return [(str(n), str(d)) for n, d in rows.itertuples(index=False, name=None)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_prompt_from_phoenix_row(\n",
    "    row: dict,\n",
    "    hotels_df: pd.DataFrame,\n",
    "    name_col: str = \" HotelName\",\n",
    "    desc_col: str = \" Description\",\n",
    "    implicit_lookup: Optional[Dict[str, str]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build an evaluation prompt for an LLM-as-a-judge from a Phoenix dataset row.\n",
    "    Includes relevance + factuality instructions, query, answer, hotel descriptions, reviews, web results,\n",
    "    and (optionally) 'implicit' explanation if present in implicit_lookup.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Query (Phoenix traces: last user message)\n",
    "    query = row[\"input\"][\"messages\"][0][-1]\n",
    "    norm_query = _normalize(str(query))\n",
    "\n",
    "    # 2) Answer\n",
    "    answer = row[\"output\"][\"messages\"][-1][\"content\"]\n",
    "\n",
    "    # 3) Citations in answer\n",
    "    _, unique_hits = extract_citations(answer, return_unique=True)\n",
    "\n",
    "    # Descriptions (from hotels_df)\n",
    "    desc_pairs = get_descriptions_from_df(unique_hits, hotels_df, name_col, desc_col)\n",
    "    descriptions = \"\\n\".join(f\"\\n- {name}: {desc.strip()}\" for name, desc in desc_pairs)\n",
    "    if not descriptions:\n",
    "        descriptions = \"(no matching hotel descriptions found)\"\n",
    "\n",
    "    # Reviews and web results (filtered)\n",
    "    allowed_names = [d[\"value\"] for d in unique_hits if d[\"type\"] == \"review\"]\n",
    "    filtered_reviews = []\n",
    "    website = []\n",
    "\n",
    "    for msg in row[\"output\"][\"messages\"]:\n",
    "        if msg.get(\"name\") == \"reviews\":\n",
    "            content = msg[\"content\"]\n",
    "            chunks = re.split(r\"\\n\\*\\*\\*\\n|\\n\\n\", content.strip())\n",
    "            for chunk in chunks:\n",
    "                try:\n",
    "                    text, json_str = chunk.rsplit(\"\\n\", 1)\n",
    "                    meta = json.loads(json_str)\n",
    "                    if meta.get(\"Name\") in allowed_names:\n",
    "                        filtered_reviews.append(chunk.strip())\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        elif msg.get(\"name\") == \"web_search\":\n",
    "            content = msg[\"content\"]\n",
    "            try:\n",
    "                data = json.loads(content)\n",
    "                website += [sample[\"content\"] for sample in data.get(\"results\", [])]\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    reviews = \"\\n\\n***\\n\\n\".join(filtered_reviews) or \"(no matching hotel reviews found)\"\n",
    "    web = \"\\n\\n***\\n\\n\".join(website) or \"(no websites found)\"\n",
    "\n",
    "    # 4) Implicit (optional)\n",
    "    implicit = None\n",
    "    if implicit_lookup:\n",
    "        implicit = implicit_lookup.get(norm_query)\n",
    "    implicit_block = implicit if implicit else \"(no implicit explanation provided)\"\n",
    "\n",
    "\n",
    "\n",
    "    # prompt_header = (\n",
    "    #     \"You are a Relevance Judge for HOTEL RECOMMENDATIONS.\\n\"\n",
    "    #     \"Evaluate ONLY using the provided hotel descriptions and reviews (ignore any outside knowledge).\\n\\n\"\n",
    "    #     \"Task: Rate how well the ANSWER addresses the USER QUERY on a 1–5 scale:\\n\"\n",
    "    #     \"1 = Not relevant at all — completely misses the user's needs.\\n\"\n",
    "    #     \"2 = Slightly relevant — touches minor aspects but not the core requirements.\\n\"\n",
    "    #     \"3 = Moderately relevant — addresses some key points but misses important requirements.\\n\"\n",
    "    #     \"4 = Very relevant — covers most requirements well, with minor omissions.\\n\"\n",
    "    #     \"5 = Perfectly relevant — comprehensively addresses all requirements with appropriate detail.\\n\\n\"\n",
    "    #     \"When evaluating, consider:\\n\"\n",
    "    #     \"• Does the answer directly address the specific hotel requirements (location, budget, amenities, dates, party size)?\\n\"\n",
    "    #     \"• Are concrete hotel recommendations provided (names + pertinent details) rather than generic advice?\\n\"\n",
    "    #     \"• Is the reasoning clear, structured, and evidence-based from the given context?\\n\"\n",
    "    #     \"• Are trade-offs/limitations explained when relevant, and is a clear recommendation given?\\n\"\n",
    "    #     \"• IMPORTANT: If no concrete hotel recommendation is made when the query calls for it, score = 1.\\n\"\n",
    "    #     # \"• IMPORTANT: Only consider claims that are supported within the given context—do not rely solely on the answer text.\\n\\n\"\n",
    "\n",
    "    #     # \"Output format: Return ONLY the integer score (1–5) with no additional text.\"\n",
    "    #     \"Output format: Return ONLY a valid JSON object with two fields:\"\n",
    "    #     \"- score: an integer from 1 to 5\"\n",
    "    #     \"- explanation: a brief explanation for the chosen score\"\n",
    "    #     \"Example:\\n\"\n",
    "    #     '{\\n  \"score\": 4,\\n  \"explanation\": \"The answer is mostly correct and relevant but misses a minor detail.\"\\n}'\n",
    "    #     \"Do not include any text outside the JSON object.\"\n",
    "    # )\n",
    "\n",
    "    prompt_header = (\n",
    "    \"You are a Factuality Judge for HOTEL RECOMMENDATIONS.\\n\"\n",
    "    \"Your goal is to assess the factual accuracy of the ANSWER strictly based on the provided hotel descriptions and reviews.\\n\"\n",
    "    \"IGNORE any outside knowledge or assumptions — only consider information verifiable from the given sources.\\n\\n\"\n",
    "    \"Task: Rate how FACTUALLY ACCURATE the ANSWER is on a 1–5 scale:\\n\"\n",
    "    \"1 = Completely inaccurate — contains mostly false or unsupported statements.\\n\"\n",
    "    \"2 = Poor factuality — some facts are correct, but most claims lack evidence or contradict the sources.\\n\"\n",
    "    \"3 = Partially factual — roughly half the claims are supported, others are vague or unverified.\\n\"\n",
    "    \"4 = Mostly factual — nearly all claims align with the sources, with only minor inaccuracies or omissions.\\n\"\n",
    "    \"5 = Fully factual — every factual statement is accurate and directly supported by a cited source.\\n\\n\"\n",
    "    \"When evaluating, consider:\\n\"\n",
    "    \"• Does each factual statement about the hotel (e.g., location, amenities, ratings, accessibility, services) have explicit evidence from the provided descriptions or reviews?\\n\"\n",
    "    \"• Are there any hallucinated details or claims not grounded in the sources?\\n\"\n",
    "    \"• Are sources cited clearly and correctly linked to each factual statement?\\n\"\n",
    "    \"• Is the information consistent with the evidence, without contradictions or exaggerations?\\n\"\n",
    "    \"• IMPORTANT: If any factual statement lacks an explicit source, deduct points proportionally.\\n\\n\"\n",
    "    \"Output format: Return ONLY a valid JSON object with two fields:\\n\"\n",
    "    \"- score: an integer from 1 to 5\\n\"\n",
    "    \"- explanation: a concise justification mentioning which parts are well-supported and which are not.\\n\\n\"\n",
    "    \"Example:\\n\"\n",
    "    '{\\n  \"score\": 4,\\n  \"explanation\": \"Most details (location, breakfast, and accessibility) are supported by the descriptions, but the mention of a rooftop bar lacks evidence.\"\\n}'\n",
    "    \"Do not include any text outside the JSON object.\"\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    final_prompt = f\"\"\"{prompt_header}\n",
    "\n",
    "**********\n",
    "User's query:\n",
    "{query}\n",
    "\n",
    "**********\n",
    "Answer:\n",
    "{answer}\n",
    "**********\n",
    "Clarification of the user:\n",
    "{implicit_block}\n",
    "\n",
    "Descriptions:\n",
    "{descriptions}\n",
    "\n",
    "Reviews:\n",
    "{reviews}\n",
    "\n",
    "Web:\n",
    "{web}\n",
    "\"\"\"\n",
    "\n",
    "    return final_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit_lookup = build_implicit_lookup(\n",
    "    exp,\n",
    "    query_col=\"query\",  \n",
    "    implicit_col='Clarification Ground Truth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from phoenix.experiments.types import Example\n",
    "from phoenix.experiments import run_experiment\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- your bedrock model (as you specified) ---\n",
    "judge_llm = init_chat_model(\n",
    "    \"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# --- helpers: robust JSON parsing from model output ---\n",
    "_JSON_RE = re.compile(r\"\\{.*\\}\", re.DOTALL)\n",
    "\n",
    "def parse_judge_json(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract the first JSON object from text and parse it.\n",
    "    Fallback to an empty schema if parsing fails.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    m = _JSON_RE.search(text)\n",
    "    candidate = m.group(0) if m else text\n",
    "    try:\n",
    "        data = json.loads(candidate)\n",
    "        assert isinstance(data, dict)\n",
    "        return data\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "def my_task(example: Example) -> dict:\n",
    "    row_like = {\"input\": example.input, \"output\": example.output}\n",
    "    prompt = build_prompt_from_phoenix_row(row_like, df, \" HotelName\", \" Description\", implicit_lookup)\n",
    "    # print(prompt)\n",
    "\n",
    "    resp = judge_llm.invoke(prompt)\n",
    "    # text = getattr(resp, \"content\", None) or getattr(resp, \"text\", None) or str(resp)\n",
    "\n",
    "    # return {\"judge_text\": text}  # super minimal\n",
    "    text = getattr(resp, \"content\", None) or getattr(resp, \"text\", None) or str(resp)\n",
    "\n",
    "    # --- inside your function ---\n",
    "    parsed = {\"score\": None, \"explanation\": None, \"raw_text\": text}\n",
    "\n",
    "    try:\n",
    "        # 1) Strip ```json fences (and any stray whitespace)\n",
    "        cleaned = re.sub(r\"^\\s*```(?:json)?\\s*|\\s*```\\s*$\", \"\", text.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "        # 2) If still fails, try extracting first {...} block\n",
    "        try:\n",
    "            j = json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r\"\\{.*\\}\", cleaned, flags=re.DOTALL)\n",
    "            if match:\n",
    "                j = json.loads(match.group(0))\n",
    "            else:\n",
    "                raise  # re-raise to go to outer except\n",
    "\n",
    "        if isinstance(j, dict):\n",
    "            try:\n",
    "                parsed[\"score\"] = max(1, min(5, int(j.get(\"score\", None))))  # clamp 1–5\n",
    "            except (TypeError, ValueError):\n",
    "                parsed[\"score\"] = None\n",
    "            parsed[\"explanation\"] = (j.get(\"explanation\") or \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        parsed[\"error\"] = f\"Failed to parse JSON: {e}\"\n",
    "\n",
    "    print(parsed)\n",
    "    return parsed\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def _int_from_text(text: str) -> float:\n",
    "    try:\n",
    "        v = int(str(text).strip())\n",
    "        return float(max(1, min(5, v)))\n",
    "    except Exception:\n",
    "        m = re.search(r\"\\b([1-5])\\b\", str(text))\n",
    "        return float(m.group(1)) if m else 1.0\n",
    "\n",
    "def relevance_metric(input, output, metadata=None) -> float:\n",
    "    # return _int_from_text(output.get(\"judge_text\", \"\"))\n",
    "    return _int_from_text(output.get(\"score\", \"\"))\n",
    "\n",
    "# If you drop factuality entirely:\n",
    "evaluators = [relevance_metric]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = client.get_dataset(name=\"agent\", version_id=\"...\")\n",
    "\n",
    "experiment = run_experiment(dataset, my_task, evaluators=evaluators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f6e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.eval_summaries[0].stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

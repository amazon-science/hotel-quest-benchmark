{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8991ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUALIFIER_PROMPT = \"\"\"\n",
    "You are a classifier that extracts qualifiers from a hotel search query and assigns two independent label sets:\n",
    "(1) Qualifier Type — how the qualifier is expressed; (2) Qualifier Content — what the qualifier is about.\n",
    "A qualifier may have only Type, only Content, or both.\n",
    "\n",
    "TASK\n",
    "1) Split the user query into the maximum number of distinct, minimal qualifier phrases possible. \n",
    "   - Each qualifier should represent a single, atomic idea, property, or constraint.\n",
    "   - Break compound statements into separate qualifiers whenever possible \n",
    "     (e.g., \"family-friendly hotel with pool and near the beach\" → \n",
    "      [\"family-friendly\", \"with pool\", \"near the beach\"]).\n",
    "2) For each qualifier, assign zero, one, or multiple labels from Qualifier Type and/or Qualifier Content.\n",
    "3) Return ONLY a valid JSON list of objects, one per qualifier.\n",
    "\n",
    "QUALIFIER TYPE (zero or more):\n",
    "- Explicit — directly stated fact or constraint\n",
    "  Examples: \"from Marriott chain\", \"must have gym\", \"with pool\"\n",
    "- Implicit — inferred preference, not literally stated\n",
    "  Examples: \"budget-friendly\", \"luxury\", \"popular hotel\"\n",
    "- Negation — excludes something\n",
    "  Example: \"not too loud\"\n",
    "- Similarity — compares to another known entity\n",
    "  Example: \"similar to Hilton Miami\"\n",
    "- Range — price/distance/numeric limit\n",
    "  Example: \"up to 60 pounds a night\"\n",
    "- Time-sensitive — urgency, trend, or recency\n",
    "  Example: \"popular hotel\", \"last-minute deal\"\n",
    "- Optional vs. Mandatory — indicates importance\n",
    "  Examples: \"optionally pet-friendly\" (Optional), \"must be wheelchair-accessible\" (Mandatory)\n",
    "- Objective — factual, measurable, or verifiable constraint\n",
    "  Examples: \"within 5 km\", \"under $100\", \"4-star rating\", \"must have gym\"\n",
    "- Subjective — opinion-based, emotional, or qualitative\n",
    "  Examples: \"quiet place\", \"romantic vibe\", \"luxury\", \"not too loud\"\n",
    "\n",
    "QUALIFIER CONTENT (zero or more):\n",
    "- Purpose — reason for the trip\n",
    "  Examples: \"solo trip\", \"business conference\"\n",
    "- Location — place/region/proximity\n",
    "  Examples: \"{City}/{State}\", \"next to the beach\", \"near TICC\"\n",
    "- Population — group type/size\n",
    "  Examples: \"for a couple\", \"for a family of 4\",\n",
    "            \"for a family of 4 with a 1 year old and a 5 year old\"\n",
    "- Seasonality — time of year/season\n",
    "  Examples: \"summer getaway\", \"monsoon-season travel\"\n",
    "- Description — amenities/property/style\n",
    "  Explicit examples: \"with gym\", \"with pool\", \"{Chain}\"\n",
    "  Implicit examples: \"luxury\", \"family-friendly\", \"cheap\"\n",
    "- Reviews — rating/reputation\n",
    "  Examples: \"4 star ratings\", \"highly recommended by backpackers\"\n",
    "\n",
    "RULES\n",
    "- Split the query as granularly as possible to maximize the number of qualifiers.\n",
    "- Keep \"text\" exactly as it appears in the query.\n",
    "- \"type\" and \"content\" MUST be present for every qualifier, but each may be an empty array [].\n",
    "- A qualifier can be both Objective and Subjective only if it contains mixed factual and opinion-based elements.\n",
    "- Do not invent information not present or reasonably implied by the query.\n",
    "- Return ONLY a JSON array of objects.\n",
    "\n",
    "EXAMPLE\n",
    "Input Query:\n",
    "\"Looking for a budget-friendly hotel near TICC for a business conference, must have gym and not too loud.\"\n",
    "\n",
    "Output:\n",
    "[\n",
    "  {\n",
    "    \"text\": \"budget-friendly\",\n",
    "    \"type\": [\"Implicit\", \"Subjective\"],\n",
    "    \"content\": [\"Description\"]\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"near TICC\",\n",
    "    \"type\": [\"Explicit\", \"Objective\"],\n",
    "    \"content\": [\"Location\"]\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"business conference\",\n",
    "    \"type\": [\"Objective\"],\n",
    "    \"content\": [\"Purpose\"]\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"must have gym\",\n",
    "    \"type\": [\"Explicit\", \"Mandatory\", \"Objective\"],\n",
    "    \"content\": [\"Description\"]\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"not too loud\",\n",
    "    \"type\": [\"Negation\", \"Subjective\"],\n",
    "    \"content\": [\"Description\"]\n",
    "  }\n",
    "]\n",
    "\n",
    "USER QUERY\n",
    "<<QUERY>>\n",
    "\"\"\"\n",
    "# Example usage:\n",
    "# prompt = QUALIFIER_PROMPT.replace(\"<<QUERY>>\", \"Looking for a quiet romantic hotel with pool and near the beach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f63bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# --- your bedrock model (as you specified) ---\n",
    "judge_llm = init_chat_model(\n",
    "    \"us.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    "    region_name=\"us-east-1\",\n",
    "    max_tokens=4096,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838bb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"final_benchmark.csv\")\n",
    "queries = df[\"query\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "answers = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    prompt = QUALIFIER_PROMPT.replace(\"<<QUERY>>\", query)\n",
    "    answer = judge_llm.invoke(prompt)\n",
    "    answers.append(answer)\n",
    "answers_content = [x.content for x in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "type_counter = Counter()\n",
    "content_counter = Counter()\n",
    "\n",
    "for item in answers_content:\n",
    "    try:\n",
    "        clean = item.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "        qualifiers = json.loads(clean)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Skipping invalid JSON: {e}\")\n",
    "        continue\n",
    "    \n",
    "    for q in qualifiers:\n",
    "        for t in q.get(\"type\", []):\n",
    "            type_counter[t] += 1\n",
    "        \n",
    "        for c in q.get(\"content\", []):\n",
    "            content_counter[c] += 1\n",
    "\n",
    "total_types = sum(type_counter.values())\n",
    "total_contents = sum(content_counter.values())\n",
    "\n",
    "print(\"=== Type Statistics ===\")\n",
    "for t, count in type_counter.most_common():\n",
    "    pct = (count / total_types) * 100 if total_types > 0 else 0\n",
    "    print(f\"{t}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n=== Content Statistics ===\")\n",
    "for c, count in content_counter.most_common():\n",
    "    pct = (count / total_contents) * 100 if total_contents > 0 else 0\n",
    "    print(f\"{c}: {count} ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "type_counter = Counter()\n",
    "content_counter = Counter()\n",
    "\n",
    "total_queries = len(answers_content)\n",
    "\n",
    "for item in answers_content:\n",
    "    try:\n",
    "        clean = item.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "        qualifiers = json.loads(clean)\n",
    "    except json.JSONDecodeError:\n",
    "        continue\n",
    "\n",
    "    types_in_query = set()\n",
    "    contents_in_query = set()\n",
    "\n",
    "    for q in qualifiers:\n",
    "        types_in_query.update(q.get(\"type\", []))\n",
    "        contents_in_query.update(q.get(\"content\", []))\n",
    "\n",
    "    for t in types_in_query:\n",
    "        type_counter[t] += 1\n",
    "    for c in contents_in_query:\n",
    "        content_counter[c] += 1\n",
    "\n",
    "print(\"=== Type Presence per Query ===\")\n",
    "for t, count in type_counter.most_common():\n",
    "    pct = (count / total_queries) * 100\n",
    "    print(f\"{t}: {count}/{total_queries} queries ({pct:.2f}%)\")\n",
    "\n",
    "print(\"\\n=== Content Presence per Query ===\")\n",
    "for c, count in content_counter.most_common():\n",
    "    pct = (count / total_queries) * 100\n",
    "    print(f\"{c}: {count}/{total_queries} queries ({pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e16c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Iterable, Optional, List, Tuple, Set\n",
    "\n",
    "def build_presence_df(\n",
    "    queries: List[str],\n",
    "    answers_content: List[str],\n",
    "    target_types: Optional[Iterable[str]] = None,\n",
    "    target_contents: Optional[Iterable[str]] = None,\n",
    "    drop_all_zero_cols: bool = False,\n",
    ") -> Tuple[pd.DataFrame, Set[str], Set[str]]:\n",
    "    \"\"\"\n",
    "    Build a dataframe with:\n",
    "      - 'query' column (aligned to input 'queries')\n",
    "      - one 0/1 column per unique 'type' and 'content' found in answers_content\n",
    "        (or restricted to target_types / target_contents if provided).\n",
    "\n",
    "    Returns (df, all_types, all_contents)\n",
    "    \"\"\"\n",
    "    # --- helpers ---\n",
    "    def strip_fences(s: str) -> str:\n",
    "        s = s.strip()\n",
    "        s = re.sub(r'^```(?:json)?\\s*', '', s)\n",
    "        s = re.sub(r'\\s*```$', '', s)\n",
    "        return s.strip()\n",
    "\n",
    "    def ensure_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        return x if isinstance(x, list) else [x]\n",
    "\n",
    "    def colify(prefix: str, name: str) -> str:\n",
    "        safe = re.sub(r'\\W+', '_', str(name).strip().lower()).strip('_')\n",
    "        return f\"{prefix}_{safe}\" if safe else f\"{prefix}_empty\"\n",
    "\n",
    "    n = min(len(queries), len(answers_content))\n",
    "    queries = queries[:n]\n",
    "    answers_content = answers_content[:n]\n",
    "\n",
    "    all_types: Set[str] = set()\n",
    "    all_contents: Set[str] = set()\n",
    "\n",
    "    parsed_cache = []  \n",
    "    for raw in answers_content:\n",
    "        try:\n",
    "            clean = strip_fences(raw)\n",
    "            data = json.loads(clean)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_cache.append((set(), set(), False))\n",
    "            continue\n",
    "\n",
    "        qualifiers = data if isinstance(data, list) else [data]\n",
    "\n",
    "        types_in_q, contents_in_q = set(), set()\n",
    "        for qobj in qualifiers:\n",
    "            if not isinstance(qobj, dict):\n",
    "                continue\n",
    "            types_in_q.update(ensure_list(qobj.get(\"type\", [])))\n",
    "            contents_in_q.update(ensure_list(qobj.get(\"content\", [])))\n",
    "\n",
    "        parsed_cache.append((types_in_q, contents_in_q, True))\n",
    "        all_types.update(types_in_q)\n",
    "        all_contents.update(contents_in_q)\n",
    "\n",
    "    if target_types is not None:\n",
    "        all_types = {t for t in all_types if t in set(target_types)}\n",
    "    if target_contents is not None:\n",
    "        all_contents = {c for c in all_contents if c in set(target_contents)}\n",
    "\n",
    "    df = pd.DataFrame({\"query\": queries})\n",
    "    type_cols = [colify(\"type\", t) for t in sorted(all_types)]\n",
    "    content_cols = [colify(\"content\", c) for c in sorted(all_contents)]\n",
    "    for col in type_cols + content_cols:\n",
    "        df[col] = 0\n",
    "\n",
    "    for i, (types_in_q, contents_in_q, valid) in enumerate(parsed_cache):\n",
    "        if not valid:\n",
    "            continue  \n",
    "        for t in types_in_q:\n",
    "            if t in all_types:\n",
    "                df.at[i, colify(\"type\", t)] = 1\n",
    "        for c in contents_in_q:\n",
    "            if c in all_contents:\n",
    "                df.at[i, colify(\"content\", c)] = 1\n",
    "\n",
    "    if drop_all_zero_cols:\n",
    "        keep = [\"query\"]\n",
    "        keep += [c for c in type_cols + content_cols if df[c].sum() > 0]\n",
    "        df = df[keep]\n",
    "\n",
    "    return df, all_types, all_contents\n",
    "\n",
    "\n",
    "\n",
    "df, all_types, all_contents = build_presence_df(queries, answers_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb67911",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv(\"final_benchmark.csv\")\n",
    "bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b243e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    df,\n",
    "    bench,\n",
    "    on=\"query\",\n",
    "    how=\"left\",    # keeps all rows from df; use 'inner' if you want only matching ones\n",
    "    suffixes=(\"\", \"_bench\")  # avoids column name collisions\n",
    ")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if \"Unnamed: 0\" in merged_df.columns:\n",
    "    merged_df = merged_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "merged_df[\"need_clarifiaction\"] = np.where(\n",
    "    merged_df[\"Clarification Ground Truth\"].notna(), 1, 0\n",
    ")\n",
    "\n",
    "complexity_map = {\n",
    "    \"Simple\": 1,\n",
    "    \"Moderate\": 2,\n",
    "    \"Complex\": 3\n",
    "}\n",
    "merged_df[\"human_complexity_rating\"] = (\n",
    "    merged_df[\"human_complexity_rating\"]\n",
    "    .map(complexity_map)\n",
    "    .fillna(0)  # Optional: replace missing or unexpected values with 0\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=[\"Clarification Ground Truth\"])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"query_word_count\"] = (\n",
    "    merged_df[\"query\"]\n",
    "    .fillna(\"\")\n",
    "    .apply(lambda x: len(str(x).split()))\n",
    ")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589df422",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_cols = [col for col in merged_df.columns if col.startswith(\"type_\")]\n",
    "merged_df[\"num_types_in_query\"] = merged_df[type_cols].sum(axis=1)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"queries_with_answers.csv\")\n",
    "complexity_df = pd.read_csv(\"dataset_v1.csv\")\n",
    "complexity_map = {\"Simple\": 0, \"Moderate\": 0, \"Complex\": 1}\n",
    "\n",
    "# part 1: keep original prompts with mapped complexity\n",
    "df1 = complexity_df.dropna(subset=[\"human_complexity_rating\"]).copy()\n",
    "df1 = df1[[\"prompt\", \"human_complexity_rating\"]]\n",
    "df1[\"complexity\"] = df1[\"human_complexity_rating\"].map(complexity_map)\n",
    "complexity_df = df1[[\"prompt\", \"complexity\"]].dropna()\n",
    "\n",
    "df = pd.concat([df, complexity_df], axis=1).drop(columns=[\"prompt\"])\n",
    "\n",
    "\n",
    "df_agent = pd.read_csv(\"exp9_eval_v3.csv\").dropna()\n",
    "df_agent2 = pd.read_csv(\"retrievals_baselines/bm25_top3_recommendations.csv\").dropna()\n",
    "df_agent3 = pd.read_csv(\"retrievals_baselines/dense_sentence-transformers-all-MiniLM-L6-v2.csv\").dropna()\n",
    "df_agent4 = pd.read_csv(\"retrievals_baselines/reranked_BAAI-bge-m3__Qwen-Qwen3-Reranker-4B.csv\").dropna()\n",
    "df_agent5 = pd.read_csv(\"retrievals_baselines/reranked_BAAI-bge-m3__Qwen-Qwen3-Reranker-0.6B.csv\").dropna()\n",
    "df_agent6 = pd.read_csv(\"exp_haiku.csv\").dropna()\n",
    "df_agent7 = pd.read_csv(\"retrievals_baselines/qwen25_32B_answers.csv\").dropna()\n",
    "\n",
    "# Convert to dict and extract judge_text as int\n",
    "df_agent6[\"relevance_score\"] = df_agent6[\"output\"].apply(lambda x: int(ast.literal_eval(x)[\"judge_text\"]))\n",
    "df_agent[\"relevance_score\"] = df_agent[\"output\"].apply(lambda x: int(ast.literal_eval(x)[\"judge_text\"]))\n",
    "\n",
    "dfs = [df_agent, df_agent2, df_agent3, df_agent4, df_agent5, df_agent6, df_agent7]\n",
    "\n",
    "# rename relevance_score column in each df\n",
    "renamed = []\n",
    "for i, d in enumerate(dfs, start=1):\n",
    "    d_temp = d[[\"relevance_score\"]].copy()\n",
    "    d_temp.rename(columns={\"relevance_score\": f\"relevance_score_{i}\"}, inplace=True)\n",
    "    renamed.append(d_temp)\n",
    "\n",
    "# concat with your base df\n",
    "df = pd.concat([df] + renamed, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c724621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "def parse_items(cell):\n",
    "    try:\n",
    "        items = ast.literal_eval(cell)   # safer than eval\n",
    "    except Exception:\n",
    "        return []\n",
    "    return items\n",
    "\n",
    "df[\"parsed\"] = df[\"answers\"].apply(parse_items)\n",
    "\n",
    "df[\"num_items\"] = df[\"parsed\"].apply(len)\n",
    "\n",
    "all_types = set()\n",
    "all_contents = set()\n",
    "for items in df[\"parsed\"]:\n",
    "    for it in items:\n",
    "        all_types.update(it.get(\"type\", []))\n",
    "        all_contents.update(it.get(\"content\", []))\n",
    "\n",
    "for t in all_types:\n",
    "    df[f\"type_{t}\"] = df[\"parsed\"].apply(lambda items: int(any(t in it.get(\"type\", []) for it in items)))\n",
    "\n",
    "for c in all_contents:\n",
    "    df[f\"content_{c}\"] = df[\"parsed\"].apply(lambda items: int(any(c in it.get(\"content\", []) for it in items)))\n",
    "\n",
    "df = df.drop(columns=[\"parsed\",\"answers\"])\n",
    "df[\"num_words\"] = [len(x.split()) for x in df[\"query\"]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu, spearmanr, kruskal\n",
    "\n",
    "def test_feature_effects(df, score_cols, feature_cols):\n",
    "    results = []\n",
    "\n",
    "    for score in score_cols:\n",
    "        for feat in feature_cols:\n",
    "            x = df[feat].dropna()\n",
    "            y = df[score].dropna()\n",
    "\n",
    "\n",
    "            common_idx = x.index.intersection(y.index)\n",
    "            x, y = x.loc[common_idx], y.loc[common_idx]\n",
    "\n",
    "            # binary feature\n",
    "            if set(x.unique()) <= {0, 1}:\n",
    "                group0 = y[x == 0]\n",
    "                group1 = y[x == 1]\n",
    "                if len(group0) > 1 and len(group1) > 1:\n",
    "                    u, p_u = mannwhitneyu(group0, group1, alternative=\"two-sided\")\n",
    "                    results.append({\n",
    "                        \"score\": score,\n",
    "                        \"feature\": feat,\n",
    "                        \"mean_0\": group0.mean(),\n",
    "                        \"mean_1\": group1.mean(),\n",
    "                        \"mannwhitney_p\": p_u\n",
    "                    })\n",
    "\n",
    "            # categorical with >2 groups\n",
    "            elif x.nunique() > 2 and x.nunique() < 20:\n",
    "                groups = [y[x == val] for val in x.unique()]\n",
    "                if all(len(g) > 1 for g in groups):\n",
    "                    stat, p_kw = kruskal(*groups)\n",
    "                    results.append({\n",
    "                        \"score\": score,\n",
    "                        \"feature\": feat,\n",
    "                        \"kruskal_p\": p_kw\n",
    "                    })\n",
    "\n",
    "            # numeric feature\n",
    "            else:\n",
    "                rho, p_rho = spearmanr(x, y)\n",
    "                results.append({\n",
    "                    \"score\": score,\n",
    "                    \"feature\": feat,\n",
    "                    \"spearman_rho\": rho,\n",
    "                    \"spearman_p\": p_rho\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b987f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [f\"relevance_score_{i}\" for i in range(1, 8)]\n",
    "feature_cols = [c for c in df.columns if c not in [\"query\"] + score_cols]\n",
    "\n",
    "res = test_feature_effects(df, score_cols, feature_cols)\n",
    "\n",
    "# keep only significant\n",
    "sig = res[\n",
    "    (res.get(\"mannwhitney_p\", 1) < 0.05) |\n",
    "    (res.get(\"kruskal_p\", 1) < 0.05) |\n",
    "    (res.get(\"spearman_p\", 1) < 0.05)\n",
    "]\n",
    "\n",
    "# map relevance_score_* → meaningful names\n",
    "SCORE_NAME_MAP = {\n",
    "    \"relevance_score_1\": \"Sonnet\",\n",
    "    \"relevance_score_2\": \"BM25\",\n",
    "    \"relevance_score_3\": \"Dense MiniLM\",\n",
    "    \"relevance_score_4\": \"BGE-M3 + Qwen Reranker (4B)\",\n",
    "    \"relevance_score_5\": \"BGE-M3 + Qwen Reranker (0.6B)\",\n",
    "    \"relevance_score_6\": \"Haiku\",\n",
    "    \"relevance_score_7\": \"Qwen2.5 32B\"\n",
    "}\n",
    "\n",
    "sig[\"score_name\"] = sig[\"score\"].map(SCORE_NAME_MAP)\n",
    "sig = sig[[\"score_name\", \"feature\"] + [c for c in sig.columns if c not in [\"score\", \"score_name\", \"feature\"]]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
